{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv\n",
    "import urllib\n",
    "from urllib.parse import quote\n",
    "\n",
    "#language identifier 1\n",
    "!pip -q install fasttext\n",
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n",
    "model_ft = fasttext.load_model(model_path)\n",
    "\n",
    "#language identifier 2\n",
    "!pip -q install langid\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True) #instantiate identifier\n",
    "\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Importing other dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import entropy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing Open Library API\n",
    "r = requests.get('https://openlibrary.org/search.json?q=subject:(\"dogs\"+OR+\"cats\")+subject:(\"Juvenile fiction\"+OR+\"Juvenile literature\")&fields=subject')\n",
    "r = r.json()\n",
    "subs = [d['subject'] for d in r['docs']] #gets the list, AKA value from k:v in subject:list dictionary\n",
    "#print(subs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1213HiZlua35"
   },
   "outputs": [],
   "source": [
    "#discipline tags is a list\n",
    "#diversity tags is a list\n",
    "#k is the number of items to return\n",
    "\n",
    "#finds results that match ANY of the first list of tags and ANY of the second list of tags\n",
    "def search_recs(discipline_tags, diversity_tags, k):\n",
    "  #encode URI\n",
    "  discipline_tags, diversity_tags = list(map(lambda x: urllib.parse.quote(x.encode(\"utf-8\")), discipline_tags)), list(map(lambda x: urllib.parse.quote(x.encode(\"utf-8\")), diversity_tags))\n",
    "  #if this ever throws errors, maybe we need to specify unicode\n",
    "\n",
    "  #exact string matching\n",
    "  discipline_tags, diversity_tags = list(map(lambda x: f\"\\\"{x}\\\"\", discipline_tags)), list(map(lambda x: f\"\\\"{x}\\\"\", diversity_tags))\n",
    "\n",
    "  #match any of the tags\n",
    "  str_disc, str_div = '+OR+'.join(discipline_tags), '+OR+'.join(diversity_tags)\n",
    "\n",
    "  print(f'https://openlibrary.org/search.json?q=subject:({str_disc})+subject:({str_div})&fields=subject&limit={k}')\n",
    "  return requests.get(f'https://openlibrary.org/search.json?q=subject:({str_disc})+subject:({str_div})&fields=author_name,title,isbn,subject&limit={k}').json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ppOHa_ZHvYyd",
    "outputId": "75b3ca58-84e1-4382-f2e8-78e70944b075"
   },
   "outputs": [],
   "source": [
    "#print(search_recs(['social themes', 'comics & graphic novels'], ['race relations', 'americans'], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qHI75FIPOoW_"
   },
   "outputs": [],
   "source": [
    "def get_tags(books):\n",
    "    r = []\n",
    "    for isbn in books:\n",
    "        response = requests.get(f'https://openlibrary.org/search.json?q=isbn:{isbn}&fields=subject').json()\n",
    "\n",
    "        if response['docs']:\n",
    "            r.append(response['docs'][0].get('subject', []))  # Use .get() to handle missing 'subject'\n",
    "        else:\n",
    "          continue\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SB-1FyiMRm2V",
    "outputId": "2e2cfe62-0d7a-4103-b800-338edf983322"
   },
   "outputs": [],
   "source": [
    "lst = get_tags([9780192832696, 9780451015594])\n",
    "#print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "iHfj9VIeW0tA"
   },
   "outputs": [],
   "source": [
    "#takes in a list of lists\n",
    "def clean_tags(tags):\n",
    "  for idx, l in enumerate(tags): #index, list of lists\n",
    "\n",
    "    #lowercase\n",
    "    l = [s.lower() for s in l]\n",
    "\n",
    "    #language identifier\n",
    "    #We can either keep a tag if both methods AGREE that it is english OR only use one and set a probability threshold for english likelihood\n",
    "    l = [s for s in l if model_ft.predict(s)[0][0] == '__label__eng_Latn'] #if english, using fast text; https://aclanthology.org/E17-2068/\n",
    "    #if english, using langid\n",
    "    l = [s for s in l if identifier.classify(s)[0] == 'en'] #off a cursory glance, performs better than the fasttext one, but still not as robust as using both; http://www.aclweb.org/anthology/P12-3005\n",
    "\n",
    "    #remove mentions of \"fiction\" to prevent stripped pertinent information due to commas later on\n",
    "    l = [s.split('in fiction')[0] for s in l] #remove any mention of 'fiction'\n",
    "    l = [s.split(', fiction')[0] for s in l] #remove any mention of 'fiction'\n",
    "    l = [s.split('fiction, ')[0] for s in l] #remove any mention of 'fiction'\n",
    "\n",
    "    #clean for extraness\n",
    "    l = [s.split(',')[0]  for s in l] #remove anything after a comma\n",
    "    l = [s.split('--')[0]  for s in l] #remove anything with the --\n",
    "    l = [s.split('(')[0]  for s in l] #remove parenthesis and anything within it\n",
    "    l = [s.split('[')[0]  for s in l] #remove parenthesis and anything within it\n",
    "    l = [s.split('{')[0]  for s in l] #remove parenthesis and anything within it\n",
    "    l = [s.split('/')[0]  for s in l] #look at info before slash\n",
    "    l = [s.split('\"')[0]  for s in l] #remove quotes\n",
    "    l = [s for s in l if \":\" not in s] #remove anything with parentheses\n",
    "    l = [s for s in l if \"reading level\" not in s] #remove any mention of reading level\n",
    "\n",
    "    #remove other uninformative tags\n",
    "    l = [s for s in l if \"translations\" not in s]\n",
    "    l = [s for s in l if \"staff\" not in s] #staff picks\n",
    "    l = [s for s in l if \"language materials\" not in s] #language materials\n",
    "\n",
    "    #remove dewey system stuff until further notice\n",
    "    l = [s for s in l if not s.isdigit()]\n",
    "\n",
    "    #ampersand in the tags is causing problems\n",
    "\n",
    "    #remove whitespace\n",
    "    l = [s.strip(' \\t\\n\\r') for s in l]\n",
    "\n",
    "    #remove empty string\n",
    "    l = [s for s in l if bool(s) != False]\n",
    "\n",
    "    #make unique, update list\n",
    "    tags[idx] = list(set(l))\n",
    "\n",
    "  return tags #list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/markayiah/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model for embeddings\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model = hub.load(module_url)\n",
    "\n",
    "print (\"module %s loaded\" % module_url)\n",
    "\n",
    "def embed(input):\n",
    "  return model(input)\n",
    "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Desired diversity area and overarching discipline\n",
    "desired_diversity = 'gender representation'\n",
    "overarching_discipline = 'african studies'\n",
    "\n",
    "# Function to categorize tags\n",
    "def categorize_tags(desired_diversity, overarching_discipline, tags):\n",
    "    categorized_tags = []\n",
    "    for tag_list in tags:\n",
    "        # Generate embeddings for tags, diversity area, and discipline\n",
    "        #now broken, need to redo; embeddings need to all be done in the same thing\n",
    "\n",
    "        embeddings = embed(tag_list, [desired_diversity], [overarching_discipline])\n",
    "\n",
    "        cat_tags = {}\n",
    "        for i, tag in enumerate(tag_list):\n",
    "            # Calculate cosine similarity to both diversity and discipline\n",
    "            diversity_sim = cosine_similarity([tag_embeddings[i]], diversity_embedding)[0][0]\n",
    "            discipline_sim = cosine_similarity([tag_embeddings[i]], discipline_embedding)[0][0]\n",
    "\n",
    "            # Categorize based on higher similarity\n",
    "            if (diversity_sim > .25) or (discipline_sim > .15):\n",
    "                if diversity_sim > discipline_sim:\n",
    "                    cat_tags[tag] = 'Diversity'\n",
    "                elif discipline_sim > diversity_sim:\n",
    "                    cat_tags[tag] = 'Discipline'\n",
    "            else:\n",
    "                cat_tags[tag] = 'Neither'\n",
    "        categorized_tags.append(cat_tags)\n",
    "\n",
    "    return categorized_tags\n",
    "\n",
    "# Categorize tags\n",
    "categorized_tags = categorize_tags(desired_diversity, overarching_discipline, cleaned_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the tags into their respective catogories\n",
    "cat_alpha, cat_beta = [], []\n",
    "for lst in categorized_tags:\n",
    "    for tag in lst:\n",
    "        if lst[tag] == 'Discipline':\n",
    "            cat_alpha.append(tag)\n",
    "        elif lst[tag] == 'Diversity':\n",
    "            cat_beta.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/markayiah/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rao's Entropy (Diversity): 0.20820375636589433\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model for topic embeddings\n",
    "\n",
    "# Define Rao's entropy formula\n",
    "def raos_entropy(cat_alpha, cat_beta):\n",
    "    # Generate embeddings for topics\n",
    "    alpha_embeddings = embed(cat_alpha)\n",
    "    beta_embeddings = embed(cat_beta)\n",
    "\n",
    "    # Full list of syllabus topics\n",
    "    syllabus_topics = cat_alpha + cat_beta\n",
    "\n",
    "    # Calculate proportions (p_i and p_j)\n",
    "    topic_counts = Counter(syllabus_topics)\n",
    "    total_topics = len(syllabus_topics)\n",
    "    p_alpha = np.array([topic_counts[topic] / total_topics for topic in cat_alpha])\n",
    "    p_beta = np.array([topic_counts[topic] / total_topics for topic in cat_beta])\n",
    "    entropy = 0.0\n",
    "    # Calculate pairwise cosine distances between topics\n",
    "    distance_matrix = cosine_distances(alpha_embeddings, beta_embeddings)\n",
    "    \n",
    "    # Sum over all topic pairs\n",
    "    for i in range(len(cat_alpha)):\n",
    "        for j in range(len(cat_beta)):\n",
    "            entropy += p_alpha[i] * p_beta[j] * distance_matrix[i, j]\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# Calculate diversity using Rao's entropy\n",
    "entropy = raos_entropy(cat_alpha, cat_beta)\n",
    "print(f\"Rao's Entropy (Diversity): {entropy}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
